{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f422a5-c47e-48ec-9cde-8e901b879c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\pablo\\anaconda3\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pablo\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pablo\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57139f9-c726-4b0f-a45c-0d89b0e7e370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformersNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/79/e1/dcba5ba74392015ceeababf3455138f5875202e66e3316d7ca223bdb7b1c/transformers-4.41.1-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.23.0 from https://files.pythonhosted.org/packages/92/27/1a30d8082ef3c8615ae198b9d451fafffdab815b96727ec3c06befc27ebe/huggingface_hub-0.23.1-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.23.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pablo\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pablo\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\pablo\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.20,>=0.19 from https://files.pythonhosted.org/packages/65/8e/6d7d72b28f22c422cff8beae10ac3c2e4376b9be721ef8167b7eecd1da62/tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata\n",
      "  Using cached tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.0->transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/ba/a3/16e9fe32187e9c8bc7f9b7bcd9728529faa725231a0c96f2f98714ff2fc5/fsspec-2024.5.0-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pablo\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pablo\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pablo\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pablo\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Using cached transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
      "Using cached huggingface_hub-0.23.1-py3-none-any.whl (401 kB)\n",
      "Using cached tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Installing collected packages: fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.15.1\n",
      "    Uninstalling huggingface-hub-0.15.1:\n",
      "      Successfully uninstalled huggingface-hub-0.15.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.2\n",
      "    Uninstalling tokenizers-0.13.2:\n",
      "      Successfully uninstalled tokenizers-0.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Acceso denegado: 'C:\\\\Users\\\\pablo\\\\anaconda3\\\\Lib\\\\site-packages\\\\~okenizers\\\\tokenizers.cp311-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a010b769-d5aa-4a56-aef4-9d72216fcdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformersNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/79/e1/dcba5ba74392015ceeababf3455138f5875202e66e3316d7ca223bdb7b1c/transformers-4.41.1-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pablo\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pablo\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\pablo\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pablo\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pablo\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pablo\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pablo\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pablo\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Using cached transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.41.1\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c3d50cf-3c65-410a-9f88-64e59da0659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69703945-5ce2-4e98-b284-e0ef08f0396e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlptown/bert-base-multilingual-uncased-sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlptown/bert-base-multilingual-uncased-sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py:39\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     40\u001b[0m         is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev,  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     _module \u001b[38;5;241m=\u001b[39m _err\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\compat\\__init__.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     pa_version_under10p1,\n\u001b[0;32m     29\u001b[0m     pa_version_under11p0,\n\u001b[0;32m     30\u001b[0m     pa_version_under13p0,\n\u001b[0;32m     31\u001b[0m     pa_version_under14p0,\n\u001b[0;32m     32\u001b[0m     pa_version_under14p1,\n\u001b[0;32m     33\u001b[0m     pa_version_under16p0,\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\compat\\pyarrow.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     _palv \u001b[38;5;241m=\u001b[39m Version(Version(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m)\u001b[38;5;241m.\u001b[39mbase_version)\n\u001b[0;32m     11\u001b[0m     pa_version_under10p1 \u001b[38;5;241m=\u001b[39m _palv \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10.0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     pa_version_under11p0 \u001b[38;5;241m=\u001b[39m _palv \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m11.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pyarrow' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\", force_download=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\", force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d976fe-a047-45ac-ac90-4b38fd4de254",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Comment_cleaned_age_gender.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f324d207-d91f-4208-93ef-de5d8fb6ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(review):\n",
    "    tokens = tokenizer.encode(review, return_tensors='pt')\n",
    "    result = model(tokens)\n",
    "    return int(torch.argmax(result.logits))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67990881-af6b-4b15-a696-6a06b4875470",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_score(df['comentario'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c17778-e2e8-4b1a-9656-582261c48114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_puntaje_sentimiento(df):\n",
    "    analysis = []\n",
    "    for comentario in df['comentario']:\n",
    "        if isinstance(comentario, str):\n",
    "            # Truncar el comentario si es demasiado largo\n",
    "            comentario_truncado = comentario[:512]  # Aquí se asume que el tamaño máximo es 512 tokens\n",
    "            tokens = tokenizer.encode(comentario_truncado, return_tensors='pt', max_length=512, truncation=True)\n",
    "            # Evaluar el modelo con el texto truncado\n",
    "            result = model(tokens)\n",
    "            # Obtener la salida deseada del modelo\n",
    "            score = torch.argmax(result.logits).item() + 1\n",
    "            analysis.append(score)\n",
    "        else:\n",
    "            analysis.append(None)  # Otra opción es asignar algún valor para comentarios que no sean cadenas de texto\n",
    "    df['analysis'] = analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1654308a-2dcb-43fa-9e74-e8f1a5d1687a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calcular_puntaje_sentimiento(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f781dcd6-dde1-4b27-80b9-b03eecfb6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diff'] = df['rating'] - df['analysis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd510d0d-0e55-4572-9ec3-0123989c40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325fb107-89ff-49b9-8e4d-b76098b5c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diff'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da81a4-bb77-45d1-8ac6-fe7f065d0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "aparts = df[['apart_id', 'comentario']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f009bf3-5b1c-460b-aa2e-a5b32ba6497b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85470166-2fc3-4fc1-83c2-5161dbd3f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Descargar los recursos necesarios de NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Definir una función para limpiar los comentarios\n",
    "def limpiar_comentario(comentario):\n",
    "    # Eliminar signos de puntuación y las palabras 'estancia' y 'días'\n",
    "    comentario_sin_puntuacion = re.sub('['+string.punctuation+']', '', comentario)\n",
    "    comentario_sin_estancia = re.sub(r'\\bestancia\\b', '', comentario_sin_puntuacion, flags=re.IGNORECASE)\n",
    "    comentario_sin_dias = re.sub(r'\\bdías\\b', '', comentario_sin_estancia, flags=re.IGNORECASE)\n",
    "    return comentario_sin_dias\n",
    "\n",
    "# Aplicar la función para limpiar los comentarios\n",
    "aparts['comentario_limpio'] = aparts['comentario'].apply(limpiar_comentario)\n",
    "\n",
    "# Obtener las stopwords en español\n",
    "stopwords_espanol = set(stopwords.words('spanish'))\n",
    "\n",
    "# Paso 1: Agrupa los comentarios por apartamento\n",
    "comentarios_por_apartamento = aparts.groupby('apart_id')['comentario_limpio'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Paso 2: Combina todos los comentarios correspondientes a cada apartamento en un solo texto\n",
    "textos_por_apartamento = comentarios_por_apartamento['comentario_limpio'].tolist()\n",
    "\n",
    "# Paso 3: Tokeniza el texto para dividirlo en palabras y filtra las stopwords\n",
    "palabras_por_apartamento = [[palabra.lower() for palabra in word_tokenize(texto) if palabra.lower() not in stopwords_espanol] for texto in textos_por_apartamento]\n",
    "\n",
    "# Paso 4: Realiza un recuento de las palabras para encontrar las más comunes\n",
    "palabras_comunes_por_apartamento = [Counter(palabras) for palabras in palabras_por_apartamento]\n",
    "\n",
    "palabras_x_apartamento = pd.DataFrame({'apart_id': comentarios_por_apartamento['apart_id'],\n",
    "    'palabras_comunes': palabras_comunes_por_apartamento})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e20499-7c2e-469f-97a0-0cc4877538d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Palabras clave a buscar en los comentarios\n",
    "palabras_clave = ['en pareja', 'con pareja', 'en familia', 'con hijos', 'con amigos', 'solo']\n",
    "\n",
    "# Paso 1: Agrupa los comentarios por apartamento\n",
    "comentarios_por_apartamento = aparts.groupby('apart_id')['comentario_limpio'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Paso 2: Combina todos los comentarios correspondientes a cada apartamento en un solo texto\n",
    "textos_por_apartamento = comentarios_por_apartamento['comentario_limpio'].tolist()\n",
    "\n",
    "# Paso 3: Busca la palabra más común entre las palabras clave en los comentarios de cada apartamento\n",
    "palabra_mas_comun_por_apartamento = []\n",
    "for texto in textos_por_apartamento:\n",
    "    contador_palabras_clave = Counter()\n",
    "    for palabra_clave in palabras_clave:\n",
    "        contador_palabras_clave[palabra_clave] = texto.count(palabra_clave)\n",
    "    palabra_mas_comun = contador_palabras_clave.most_common(1)\n",
    "    palabra_mas_comun_por_apartamento.append(palabra_mas_comun[0][0] if palabra_mas_comun else None)\n",
    "\n",
    "# Crear un DataFrame con los resultados\n",
    "df_palabra_mas_comun = pd.DataFrame(palabra_mas_comun_por_apartamento, columns=['Palabra Más Común'])\n",
    "\n",
    "# Agregar el id del apartamento como índice\n",
    "df_palabra_mas_comun['apart_id'] = comentarios_por_apartamento['apart_id']\n",
    "df_palabra_mas_comun = df_palabra_mas_comun.set_index('apart_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f369d-8cb0-4bd8-92a5-80114e2a6ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_palabra_mas_comun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287a803-c810-4515-8c60-f980cc144dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_palabra_mas_comun['Palabra Más Común'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7387c10a-5cdb-4834-a858-d8e4a461ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_palabra_mas_comun.to_csv('CompaniaApart.csv')\n",
    "palabras_x_apartamento.to_csv('PalabrasApart.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
